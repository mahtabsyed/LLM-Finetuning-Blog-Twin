{"text": "### Instruction:\nWrite about \u201cAI Software Engineering\u201d (AI + SE = AISE) in your own style\n\n### Response:\n\u201cAI Software Engineering\u201d (AI + SE = AISE)\n\nMahtab Syed\n\nData and AI Leader | AI Solutions | Cloud Architecture(Azure, GCP, AWS) | Data Engineering, Generative AI, Artificial Intelligence, Machine Learning and MLOps Programs | Coding and Kaggle\n\nDecember 27, 2025\n\nThis blog is my summary of learnings over the last 2 years \n\nfrom taking help from Large Language Models for code generation\n\nto full Software Engineering Life Cycle (SDLC) (meaning Requirements -> Solution Design -> Coding -> Testing -> Deploying and improving iteratively)\n\nFirst on the term \u201cVibe Coding\u201d\n\nI have deep respect for Andrej Karpathy who coined the term \u201cVibe Coding\u201d, but I am not a fan of this term, as it can be perceived very differently.\n\nPerceptions differ based on background and experience, and many of us think we can just use AI tools to generate code by typing or whispering a line e.g., \u201cMake a React JS web app which is my professional website with my blogs, projects, news, and run it in local, after that I will take your help to deploy the webapp on Azure cloud\u201d. Yes, this will work, but such one-liners will not work for enterprise apps, which are supposed to run on production, and a team of 10 software engineers have been developing it and have already written 10,000 lines of code.\n\nAnd Andrej is an expert and when he says \u201cVibe Coding\u201d many of us do not understand what he means by it, and how he practices it daily for large apps. I hope Andrej will document it one day.\n\nAnd its just today 27 Dec 2025 , Andrej has rocked the Software Engineering world with this confession (I saw this tweet after I published v1 of this blog). He is definitely million years ahead of when he coined the term \"vibe coding\" - https://x.com/karpathy/status/2004607146781278521\n\n\u201cAI Software Engineering\u201d (AISE) pronounced like ACE\n\nA more representative term is \u201cAI Software Engineering\u201d, and yes, we can use AI tools to do more than 80% of Software Engineering Life Cycle (SDLC) \u201cRequirements -> Solution Design -> Coding -> Testing -> Deploying\u201d with more than 80% accuracy and at least 2x the speed. (This is the stats from my experience and its getting better daily). But we need an experienced one-person army who can \u201cTalk to Users, Develop Empathy with Business and is Product Manager and is a Solution Architect and is a Senior AI Software Engineer\u201d. Refer my post https://www.linkedin.com/feed/update/urn:li:activity:7408083690754412545/\n\nAnd the reason I call it AI Software Engineering and not AI Assisted is its evolving so fast that AI is doing most and humans are just for final review. Check this post by Boris Cherny - https://x.com/bcherny/status/2004626064187031831?s=20\n\nHow I started and what I have been doing over the last 2 years (in chronological order)\n\nIn 2023 and 2024 I used LLMs like ChatGPT and then Google Gemini and Anthropic Claude for code generation. With good prompts, it works sometimes and messes up sometimes. If you don\u2019t understand the code well, and you get a defect and ask LLMs to change blindly, after few trials you will not be able to understand the code and LLMs can screw up your code beyond recognition.\n\nIn earlier versions of LLMs this didn\u2019t work very well, and from LLM to LLM the persona was different. So how do you control the central theme of your product i.e. standard naming conventions, code structure, files structure? If not controlled it will become patchy and cannot be understood and maintained. With larger context window in 2024 this has gotten better.\n\nWith thinking / reasoning models of OpenAI o1\u2026 o4, and Claude Sonnet and Opus, Gemini Flash and 3 Pro, the code generated with full context and improved reasoning is much better.\n\nThen I used GitHub Copilot and I should admit back in 2023-2024 GitHub Copilot was the best AI app, and one of the most adopted AI App after ChatGPT and Google Maps (my view without data to prove it)\n\nGitHub Copilot gave the inspiration to many more apps and Cursor is one very famous app. I tried it in 2024 including the Agentic option where it could auto change my code and auto run command line tools. It was good, but I discontinued Cursor, as its latency was poor for some reason, when using from Melbourne in evenings which was my main time window to experiment after work. This must have gotten better in last 6 months. I plan to try Cursor well again.\n\nAt the end of 2024, I did the \u201cGenerative AI for Software Development\u201d course by Laurence Moroney for https://www.deeplearning.ai/courses/generative-ai-for-software-development/ which had very complex coding assignments e.g. \u201cImplementing algorithms of Graphs that needed to find the shortest path for a 1000 node graph in less than 1.3 secs.\u201d. It\u2019s not just about a functional solution but a solution whose non-functional performance is best in class. I can challenge the best software engineers to try this\u2026 I tried it coding and submitting 20 times and succeeded in 21st time with the help of the best reasoning model available then in Oct 2024. I completed every assignment of this 4-month long course with 100% marks and without AI to help, I would have given up. So, this marked a transition in my thinking, and I got convinced that \u201cAI Software Engineering\u201d is here to stay and the benefit is primarily for Software Engineers and Solution Designers who understand it deep.\n\nOne key UI design issue I faced in GitHub Copilot and Cursor is the screen space in VS Code, with (1) Coding window (2) AI completion window (3) Terminal window and (4) Files - leaves little room and impacts deep work (for me). \n\nAnd then I tried Claude Code in 2025 and I am using it now :) It is very good. I use Claude CLI in one monitor and VS Code in another monitor, and both give a good perspective. Details on Claude Code and why I like it below\u2026\n\nI also tried OpenAI Codex, yes, it\u2019s good that it starts from a GitHub repo but I am not a fan of the experience. Code should run on my local laptop and not on the OpenAI Cloud. I think Claude Code has done a better job by separating Claude CLI and Coding IDE and all running local. Yes, LLM query would go to an API on Cloud. (Can this be changed to a local LLM in my laptop? To test this in the future\u2026)\n\nIf you understand LLMs are already very good at code generation, so a tool which is good at User Experience, Context Engineering, has multiple Tools, and is a collection of multiple Agents is what we need. The LLM layer is already available since 2024, and the App layer or Agentic layer above the LLM layer is the missing piece\u2026 And Claude Code is good at it today, tomorrow others can make this experience better\u2026  \n\nOn Claude Code \u2013 Key features\n\nAgentic Workflow \u2013 It\u2019s a very good example of when Agents should be deterministic in some cases and non-deterministic otherwise\n\nUses various Tools - for navigating the code base i.e. searching and finding patterns, reading and editing files, web navigation, creating sub agents, executing bash commands, planning before building, following a list of TODOs, etc\n\nMemory across sessions shared by team and individual ones for an engineer, Conversation history\n\nContext management \u2013 pointing specific files in context, clearing context, summary context\n\nCan Discover, Design, Build, Deploy, Support and Scale, work with GitHub\n\nIs Secure and Extensible\n\nCan accept screen shots to get a visual cue when typing is not sufficient, or even better using playwright you can do this automatically :) This is an amazing automation feature. \n\nMy best practices for staying in control while practicing \u201cAI Assisted Software Engineering\u201d\n\nFor a new product - Start with AI Assisted Requirements, Design, documented Design Decisions in a Design.md file which goes in the context of the LLM all the time. File structure, Variables Naming conventions, Package management tool (uv vs. pip) all go in the .md file as well.\n\nTest Driven Development - Develop comprehensive Test cases for functional and non-functional features. Yes, AI generates Test cases well and add the need for test edge cases in your .md file.\n\nAlways develop on a branch - Any Feature addition should be done in a branch and reviewed by a human and tested extensively; on failure, try to debug and if its taking time delete the branch. With AI debugging can take more time than developing, but nevertheless we need to understand what\u2019s going on. If you cannot debug it\u2019s a red flag that you are not following the process well!\n\nFor existing code when you clone a GitHub repo \u2013 first spend time understanding the full repo. AI can be a better code explainer than a Software Engineer, so use it to understand complex code base.\n\nIn general break the big task to small steps and go step by step \u2013 developing in branch, reviewing, testing, pull request, peer reviewing, then merging to main.\n\nIf not happy with the AI generated code delete the branch, and think and provide better context and start again\u2026\n\n Summary\n\nAI Software Engineering (AISE) is a skill to be practiced and refined through experience and intuition \n\nTreat AI-generated output like any team member's work\u2014question the reasoning, review thoroughly\n\nShip what passes testing; discard what doesn't\n\nAI augments my judgement; it doesn't replace it\n\nWhat goes to production is my responsibility\u2014including the bugs\n\n Acknowledgements\n\nLaurence Moroney for the best-in-class courses, his decades of experience speaks\n\nAndrew Ng for inspiring me to learn Machine Learning back in 2020 (I did his course 3 times :) to understand back propagation in neural nets well)\n\nAndrej Karpathy for the calm demeanour with deep thinking and deep inspiration \n\nElie Schoppik for the Claude Code \u2013 DeepLearning.ai course\n\nJohn Crickett  for this post which is the best summary - https://www.linkedin.com/posts/johncrickett_i-dont-mind-if-you-let-an-llm-write-all-activity-7403787961605197824-toov \n\nMahtab Syed - Melbourne - 27 Dec 2025"}
{"text": "### Instruction:\nWrite about LLM Supervised Finetuning: Theory and Practice in your own style\n\n### Response:\nLLM Supervised Finetuning: Theory and Practice\n\nMahtab Syed\n\nData and AI Leader | AI Solutions | Cloud Architecture(Azure, GCP, AWS) | Data Engineering, Generative AI, Artificial Intelligence, Machine Learning and MLOps Programs | Coding and Kaggle\n\nJanuary 2, 2026\n\nSupervised Finetuning (SFT) is the bridge between a general-purpose \"base\" model and a specialized AI assistant. While pretraining teaches a model what language is, finetuning teaches it how to behave for specific tasks.\n\nRefer code here - https://github.com/mahtabsyed/LLM-Finetuning-Blog-Twin\n\nPart 1: The Theory of Supervised Finetuning\n\nUnderstanding Post-Training\n\nPost-training involves modifying the weights of a pre-trained model to adapt it to new domains. Once a model undergoes post-training, it becomes a distinct entity from its foundational version, requiring its own deployment endpoint.\n\nModern LLM development typically follows these five stages:\n\nPretraining: Learning from massive datasets (web crawl, books).\n\nSupervised Finetuning (SFT): Instruction following and domain adaptation.\n\nRLHF / Preference Optimization: Aligning outputs with human values.\n\nReasoning Training: Enhancing \"Chain of Thought\" capabilities.\n\nDistillation: Creating smaller, efficient versions of larger models.\n\nWhy SFT via LoRA is good\n\nParameter-Efficient Finetuning (PEFT) using Low-Rank Adaptation (LoRA) has revolutionized how we update models. Instead of recalculating billions of parameters, LoRA freezes the original weights and adds small \"adapter\" matrices.\n\nEfficiency: Reduces trainable parameters by up to 10,000x.\n\nMemory: Lowers GPU requirements by up to 3x.\n\nPerformance: Achieves results comparable to full finetuning without adding inference latency.\n\nThe Finetuning Workflow with Unsloth.ai\n\nUnsloth.ai is a leading framework for training open-weight models like Gemma and Llama . It optimizes the training process for local hardware or managed services.\n\nModel Loading: Initialize the 4-bit quantized version of the base model and its tokenizer.\n\nAdapter Configuration: Attach LoRA layers by specifying the rank (matrix size) and target layers (typically Q, K, V matrices).\n\nData Formatting: Ensure your instruction-response pairs match the specific prompt templates (e.g., Alpaca or ChatML) supported by the framework.\n\nExecution: Launch the training on a GPU-enabled machine.\n\nExport: Merge the adapter weights back into the base model and push the final weights to Hugging Face for deployment.\n\nPart 2: Hands-on Implementation \u2014 The \"Blogging Twin\"\n\nTo see SFT in action, we can look at a practical project: The LLM Blogging Twin. This project demonstrates how to take a lightweight model (Llama 3.2: 1B) and train it to mimic a specific writing style.\n\nProject Overview\n\nThe goal is to create an AI assistant that writes exactly like you by feeding it your previous blog posts.\n\nKey Features\n\nData Ingestion: Automatically processes Markdown, Text, and Word docs.\n\nOptimized Pipeline: Uses Unsloth.ai to make finetuning accessible on consumer-grade GPUs.\n\nLocal Deployment: Once trained, the model is exported and run locally using Ollama.\n\nFull-Stack Experience: Includes a FastAPI backend and a React frontend to chat with your \"Twin.\"\n\nProject with an end to end process\n\nThis implementation covers the entire AI Lifecycle:\n\nData Prep: Cleaning and formatting raw text into training pairs.\n\nMLOps: Building modular pipelines and CLI tools for training.\n\nEngineering: Bridging the gap between a raw model file and a functional web application.\n\nEvaluation: Comparing the \"Base\" Llama model against your \"Finetuned\" twin to measure style accuracy.\n\nBy following this approach, you move beyond just using AI\u2014you begin crafting it to suit your unique voice and professional needs.\n\n- Mahtab Syed, Melbourne, 02 Jan 2026"}
{"text": "### Instruction:\nCompose a blog post on the topic of Large Language Model and Transformer Architecture in simple English\n\n### Response:\nLarge Language Model and Transformer Architecture in simple English\n\nMahtab Syed\n\nData and AI Leader | AI Solutions | Cloud Architecture(Azure, GCP, AWS) | Data Engineering, Generative AI, Artificial Intelligence, Machine Learning and MLOps Programs | Coding and Kaggle\n\nOctober 12, 2025\n\nTransformer is a versatile multimodal architecture, and works for text, images, audio etc.\n\nFor this article my context is text based Large Language Models with Transformer as the architecture. And will talk about\n\nHow a Large Language Model is trained to create a Base Model and an Instruct / Assistant Model\n\nHow the LLM Transformer works?\n\nKey terms:\n\n1 - Token - the smallest unit of text a model processes \u2014 it can be a whole word, part of a word, or even punctuation or space, depending on how the tokenizer splits the text. Try Tiktokenizer: https://tiktokenizer.vercel.app/\n\n\"Hello\" \u2192 1 token\n\n\"AI, rocks!\" \u2192 \"AI\" + \",\" + \"rocks\" + \"!\" \u2192 4 tokens\n\nNote - GPT-4 uses vocabulary size of 100277 tokens\n\n3 - Context length \u2013 The max number of tokes you can input to an LLM\n\nPart 1 - How an LLM is trained?\n\n3 Stages of training a LLM \n\nStage 1: Pretraining - Get a Base LLM\n\nDownload ~10TB of text from publicly available internet data. Refer - FineWeb (pretraining dataset): https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1\n\nGet a cluster of ~6,000 GPUs.\n\nTrain an LLM base model using this dataset \u2014 all the text from the internet.\n\nIn simple words, this training extracts key relationships from the text into a neural network. Cost: ~$2M | Time: ~15 days.\n\nObtain Base model e.g. GPT-3 davinci, Llama 2 70B\n\nNote - Its not common to find a Base model and you can experiment with this https://huggingface.co/spaces/Bradarr/Gemma-3-pt-llamacpp \n\nIf you enter \"The cat sat on the __\", it will keep generating the next best token - See one generation below.\n\nThis base model acts like an internet text simulator\n\nRepeat every 6 months to 1 year to obtain an updated base model.\n\nStage 2: Supervised Finetuning - Get an Instruct / Assistant LLM \n\nWrite labeling instructions.\n\nUse a dataset of high-quality conversations between humans and assistants.\n\nHire people to collect ~100K high-quality Q&A responses and/or comparisons.\n\nFinetune the base model on this dataset. Time: ~1 day.\n\nObtain Instruct / Assistant model e.g. Llama 2 70B-Chat, Mistral 7B Instruct v0.2\n\nNote - ChatGpt, Claude, Gemini app is a Web UI layer above their Instruct / Assistant model\n\nIf you enter \"The cat sat on the __\", it will keep generate the next best tokens as if answering a question as an assistant - See one generation below.\n\nWhen you interact with an Instruct / Assistant model it tries to be helpful and answers as a guide, and this caught everyones attention.\n\nRun extensive evaluations.\n\nDeploy, monitor, collect misbehaviours, and repeat the cycle.\n\nTypically done weekly.\n\nStage 3: Reinforcement Learning from Human Feedback (RLHF) - Improve the LLM\n\nApply classification, reward modelling, and reinforcement learning.\n\nThe fine-tuned model is further polished using RLHF to make it customer appropriate.\n\nStage 4: Train a Reasoning Model - Get Reasoning model\n\nSupervised Finetuning using Synthetic Data generated from Stage 3\n\nPure Reinforcement Learning (RL) to enhance Reasoning skills\n\nRejection sampling \u2013 Choosing the best examples from a distribution of data created during the last successful Reinforcement Learning (RL)\n\nSupervised Finetuning using Synthetic data and Supervised data in domains like writing, factual question answering and self-cognition.\n\nFinal Reinforcement Learning (RL) across diverse prompts and scenarios\n\nStage 5: Create smaller distilled models - Get Small Models\n\nCreate smaller models which can be used in modest hardware while retaining much of the original models reasoning capabilities\n\nPart 2 - Key features of Transformer Architecture\n\nTransformer Architecture\n\n1.\u00a0\u00a0\u00a0 Transformer is a Software system which takes text and outputs text: Input text - > Output text\n\n2.\u00a0\u00a0\u00a0 Input text could be multiple tokens, Output just one token at a time \u2013 Each next token generation is after one forward pass (the sequential layers of computations) through the model. e.g. assume the Input is : \"The cat __\" and one possible output is \"sat\"\n\n3.\u00a0\u00a0\u00a0 After each token generation the input text is appended with the generated token. This is called Autoregressive model so the next input can be be. \"The cat sat __\" \n\nIf you enter \"The cat sat on __\", it will keep generating the next best token - See one example generation in the image below.\n\n4.\u00a0\u00a0\u00a0 This loop of generating next word continues till the max_tokens is met or till a logical end is met. (This can be trained by Supervised Finetuning of an LLM model to become an Instruct / Assistant model)\n\n5.\u00a0\u00a0\u00a0 The method of choosing a single token from the probability distribution of all tokens in the vocabulary is called decoding strategy.\n\n6.\u00a0\u00a0\u00a0 One of the most compelling features of Transformer is it can process many tokens (as per context length) in parallel. If we entered the text \u201cThe cat sat on the __\u201d, The transformer in parallel processes all the tokens in this sentence, and the outputs are used in the Self-Attention layer.\n\n7.\u00a0\u00a0\u00a0 Each Transformer block consists of two main components: Self attention (allows tokens to focus on relevant words) and Feed Forward Network (processes each token's representation independently).\n\n8.\u00a0\u00a0\u00a0 Positional Encoding is added to input embeddings because Transformers have no inherent sense of word order - this encoding helps the model understand the position of each token in the sequence.\n\n9.\u00a0\u00a0\u00a0 Residual connections (skip connections) and Layer Normalization are used around each sub-component to enable stable training of deep networks and help with gradient flow.\n\n10.\u00a0\u00a0\u00a0 Context Window: Transformers have a maximum sequence length they can process (e.g., 4K, 8K, 128K tokens), which determines how much text they can \"remember\" and attend to at once.\n\n11.\u00a0\u00a0\u00a0 During training, all tokens are processed in parallel for efficiency. During inference (generation), tokens are generated sequentially one at a time in the autoregressive loop.\n\nAcknowledgements\n\nVideos and Articles - Andrej Karpathy\n\nBook Hands-On Large Language Models - Jay Alammar and Maarten Grootendorst \n\nBase Model Generation example\n\nInstruct / Assistant Model generation example\n\nMahtab Syed, Melbourne, 12 Oct 2025"}
{"text": "### Instruction:\nWrite about AI Agents or Agentic Systems in your own style\n\n### Response:\nAI Agents or Agentic Systems\n\nMahtab Syed\n\nData and AI Leader | AI Solutions | Cloud Architecture(Azure, GCP, AWS) | Data Engineering, Generative AI, Artificial Intelligence, Machine Learning and MLOps Programs | Coding and Kaggle\n\nMarch 10, 2025\n\nIn the new year 2025 we see everyone talking about \u201cAgents\u201d or Agent like systems called \u201cAgentic Systems\u201d.\n\nI recently read a post with title \u201cSaaS apps are dying, and Agents will take over\u201d. This is just another clickbait, and I don\u2019t agree to this. The Agents are not ready for this yet (as of Mar 2025), but this will change.\n\nSo, let\u2019s describe Agents and the production readiness, in simple words.\n\nWhat\u2019s an AI Agent?\n\nAn AI agent is a system that perceives its environment, processes information, and takes actions to achieve specific goals autonomously or semi-autonomously. E.g.\n\n1. My Personal Travel Agent to which if I give this chat command \u201cCan you book me a 1 week vacation to Italy in Sep 2025\u201d. This can invoke the AI Agent for these Tasks\n\na.\u00a0\u00a0\u00a0 Search best flight tickets in any week of Sep 2025\n\nb.\u00a0\u00a0\u00a0 Search best hotel accommodation in days between flights\n\nc.\u00a0\u00a0\u00a0\u00a0 Find best places to visit and book its tickets\n\nd.\u00a0\u00a0\u00a0 Send email to my work manager at work to approve the holiday, and wait for confirmation\n\ne.\u00a0\u00a0\u00a0 And on confirmation book flights, hotel and places of interest after confirming with me in a chat (so it\u2019s a semi-autonomous agent where I am the Human Reviewer). Note this involves Financial transactions which I have authorised this Agent to do\n\nf.\u00a0\u00a0\u00a0\u00a0\u00a0 Block my work calendar and put out an out of office\n\ng.\u00a0\u00a0\u00a0 Keep an eye on any flight / hotel changes, cancellation and update after confirming with me\n\nh.\u00a0\u00a0\u00a0 Etc\n\n2. My Software Engineer Agent which takes a chat like, \u201cIn the GitHub repository \"TheBestCasuals - Retail for casual clothes\u201d please replace current search feature with a new search feature which is based on latest Semantic Search technology using RAG and a Vector Db\u201d, This can be another semi-autonomous Agent which will\n\na.\u00a0\u00a0\u00a0 Get the repository \u201cTheBestCasuals - Retail for casual clothes\u201dand understand the full code\n\nb.\u00a0\u00a0\u00a0 Find current Fixed Search implementation\n\nc.\u00a0\u00a0\u00a0\u00a0 Replace this with the new logic of SemanticSearch by suggesting me the best LLM and Vector DB to use by writing all Dev and Test code\n\nd.\u00a0\u00a0\u00a0 Write best Evaluation metrics\n\ne.\u00a0\u00a0\u00a0 Creating all changes as multiple GitHub Pull Requests and send for my Review\n\nf.\u00a0\u00a0\u00a0 After I review the Code is published to the QA Env\n\ng.\u00a0\u00a0\u00a0 After I approve the Code is published to the Prod Env\n\nWhat does an Agent comprise of?\n\nAgents or Agentic Systems have an Agent Runtime which is the environment where an agent operates. It includes the Orchestration, Model, Tools, and Memory (short-term and long-term). It manages the execution of the agent, including how it takes in information, reasons about it, and uses tools to interact with the world.\n\nAgent Runtime\n\n1. Orchestration - a cyclical process that governs how the agent takes in information, performs some internal reasoning, and uses that reasoning to inform its next action or decision.\n\n2. Model - centralised decision maker for agent processes like the latest reasoning based LLMs \n\n3. Tools \u2013 access external data and services via Functions, Extensions, Data Stores and Plugins, Rest APIs etc. \n\n4. Memory - Agents manage memory using two types: Short-term and Long-term.\n\na. Short-term memory: This is immediate and context-driven, like remembering the current stage of a conversation or the last tool used.\n\nb. Long-term memory: This stores broader knowledge and learned patterns, similar to how a model's training data provides it with a foundation of understanding. Long-term memory enables an agent to draw on past experiences and apply them to new situations, enhancing its reasoning and decision-making. \n\nImage credit \u2013 Agents Google Whitepaper \n\nNow the most important question :)\n\nIf Agents are so good what\u2019s stopping us from using them for most day to day tasks?\n\nAs Chip Huyen explains in her talk there a \u201cCurse of complexity\u201d. As the no of steps in a Task increases the probability of failure increases exponentially.\n\nhttps://www.youtube.com/watch?v=D6v5rlqUIc8&ab_channel=AIEngineer\n\nThe probability of failure of an AI Agent is directly proportional to the number of steps it involves\n\n1.\u00a0\u00a0\u00a0 A relatively simple task using an AI Agent comprises of at least 5-10 steps. At the moment in Mar 2025, the LLM models can fail with a probability of approximately 2% for 1 step task, 18% for a 10 step task and 87% for a 100 step task.\n\n2.\u00a0\u00a0\u00a0 So the probability of failure of a 10 step task which is about 18% is not an acceptable probability of failure for any production grade task. The above example of \u201cMy Personal Travel Agent\u201d is more than 10 steps which means its probability of failure is more than 20%. And this Agent involves finances, booking tickets, booking hotels, so we are not ready yet.\n\nThe good news is LLMs are evolving:\n\n1.\u00a0\u00a0\u00a0 LLMs are getting better at planning and reasoning,\n\n2.\u00a0\u00a0\u00a0They can use reliable tools and\n\n3.\u00a0\u00a0\u00a0 And they are getting much larger context window.\n\nSo, it\u2019s a matter of time when Agents or Agentic Systems can be used for solving day to day production grade tasks which are slow and costly today\u2026\n\nReference and acknowledgements:\n\n1 - Agents Google Whitepaper by Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic - https://www.kaggle.com/whitepaper-agents\n\n 2 - Chip Huyen\n\nhttps://www.youtube.com/watch?v=D6v5rlqUIc8&ab_channel=AIEngineer\n\nhttps://huyenchip.com/2025/01/07/agents.html\n\n- Mahtab Syed, 10 Mar 2025, Melbourne"}
