{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Aware Fine-Tuning Test Notebook\n",
    "\n",
    "This notebook tests GPU detection and fine-tuning with either unsloth (NVIDIA/AMD/Intel) or unsloth-mlx (Apple Silicon).\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Detects your hardware platform (Apple Silicon vs CUDA GPU)\n",
    "2. Installs the appropriate fine-tuning library\n",
    "3. Loads base llama3.2:1b model\n",
    "4. Runs baseline inference BEFORE fine-tuning\n",
    "5. Fine-tunes on 2-3 simple examples\n",
    "6. Runs inference AFTER fine-tuning to show the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: GPU Detection\n",
    "\n",
    "Detect the hardware platform to determine which library to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE DETECTION\n",
      "============================================================\n",
      "System: Darwin\n",
      "Machine: arm64\n",
      "Processor: arm\n",
      "Python: 3.13.9 (main, Oct 14 2025, 21:10:40) [Clang 20.1.4 ]\n",
      "\n",
      "Detected platform: APPLE_SILICON\n",
      "============================================================\n",
      "\n",
      "Will use: unsloth-mlx\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "def detect_hardware():\n",
    "    \"\"\"\n",
    "    Detect the hardware platform for fine-tuning.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'apple_silicon', 'cuda', or 'cpu'\n",
    "    \"\"\"\n",
    "    # Check for Apple Silicon (M1/M2/M3/M4/M5)\n",
    "    if platform.system() == 'Darwin' and platform.machine() == 'arm64':\n",
    "        return 'apple_silicon'\n",
    "    \n",
    "    # Check for NVIDIA CUDA (requires torch, so we'll check this after importing torch)\n",
    "    # For now, if not Apple Silicon, assume CUDA or will detect later\n",
    "    return 'unknown'\n",
    "\n",
    "# Detect platform\n",
    "platform_type = detect_hardware()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"System: {platform.system()}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"\\nDetected platform: {platform_type.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Now check for CUDA if not Apple Silicon\n",
    "if platform_type == 'unknown':\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            platform_type = 'cuda'\n",
    "            print(f\"\\n✓ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "        else:\n",
    "            platform_type = 'cpu'\n",
    "            print(\"\\n⚠ No CUDA GPU found\")\n",
    "    except ImportError:\n",
    "        print(\"\\n⚠ PyTorch not installed yet, will install with unsloth\")\n",
    "        platform_type = 'cpu'\n",
    "\n",
    "print(f\"\\nWill use: {'unsloth-mlx' if platform_type == 'apple_silicon' else 'unsloth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Dependencies\n",
    "\n",
    "Install the appropriate fine-tuning library based on detected hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "\n",
      "Installing unsloth-mlx for Apple Silicon...\n",
      "zsh:1: command not found: pip\n",
      "✓ unsloth-mlx installed\n",
      "\n",
      "Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "if platform_type == 'apple_silicon':\n",
    "    print(\"Installing unsloth-mlx for Apple Silicon...\")\n",
    "    !pip install -q unsloth-mlx mlx datasets\n",
    "    print(\"✓ unsloth-mlx installed\")\n",
    "else:\n",
    "    print(\"Installing unsloth for CUDA/CPU...\")\n",
    "    !pip install -q \"unsloth[colab-new]\" datasets\n",
    "    print(\"✓ unsloth installed\")\n",
    "\n",
    "print(\"\\nDependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Import Libraries & Configure\n",
    "\n",
    "Import the appropriate library and configure model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "\n",
      "✓ Using unsloth-mlx with model: mlx-community/Llama-3.2-1B-Instruct-4bit\n",
      "\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Importing libraries...\\n\")\n",
    "\n",
    "if platform_type == 'apple_silicon':\n",
    "    from unsloth_mlx import FastLanguageModel\n",
    "    model_name = \"mlx-community/Llama-3.2-1B-Instruct-4bit\"\n",
    "    print(f\"✓ Using unsloth-mlx with model: {model_name}\")\n",
    "else:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model_name = \"unsloth/llama-3.2-1b\"\n",
    "    print(f\"✓ Using unsloth with model: {model_name}\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Base Model\n",
    "\n",
    "Load the base llama3.2:1b model with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "\n",
      "Model: mlx-community/Llama-3.2-1B-Instruct-4bit\n",
      "Max sequence length: 512\n",
      "Quantization: 4-bit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425c9f747da2404ab3bf72623ba9197c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base model loaded successfully!\n",
      "  Model type: MLXModelWrapper\n",
      "  Tokenizer vocab size: 128000\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\\n\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Max sequence length: 512\")\n",
    "print(f\"Quantization: 4-bit\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Base model loaded successfully!\")\n",
    "print(f\"  Model type: {type(model).__name__}\")\n",
    "print(f\"  Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Baseline Inference (BEFORE Fine-tuning)\n",
    "\n",
    "Test the base model with sample prompts to see its default behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE INFERENCE (Before Fine-tuning)\n",
      "============================================================\n",
      "Inference mode enabled with KV caching\n",
      "\n",
      "Prompt: Complete this sentence: The sky is\n",
      "Response: _______ blue today.\n",
      "\n",
      "A) A deep shade of blue\n",
      "B) A light blue\n",
      "C\n",
      "\n",
      "Prompt: Complete this sentence: Water is\n",
      "Response: life, but it's not the only thing that makes us whole.\n",
      "\n",
      "What does this sentence mean?\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE INFERENCE (Before Fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Complete this sentence: The sky is\",\n",
    "    \"Complete this sentence: Water is\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    # MLX generate expects prompt string directly, not tokenized inputs\n",
    "    response = model.generate(prompt=prompt, max_tokens=20)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Simple Training Dataset\n",
    "\n",
    "Create a minimal dataset with 3 instruction-response pairs for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n",
      "\n",
      "✓ Created dataset with 3 examples\n",
      "\n",
      "Sample training example:\n",
      "------------------------------------------------------------\n",
      "### Instruction:\n",
      "Complete the sentence\n",
      "\n",
      "### Input:\n",
      "The sky is\n",
      "\n",
      "### Response:\n",
      "blue and beautiful, stretching endlessly above us.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training dataset...\\n\")\n",
    "\n",
    "# Simple training data in Alpaca format\n",
    "training_data = [\n",
    "    {\n",
    "        \"text\": \"\"\"### Instruction:\n",
    "Complete the sentence\n",
    "\n",
    "### Input:\n",
    "The sky is\n",
    "\n",
    "### Response:\n",
    "blue and beautiful, stretching endlessly above us.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"### Instruction:\n",
    "Complete the sentence\n",
    "\n",
    "### Input:\n",
    "Water is\n",
    "\n",
    "### Response:\n",
    "essential for all life on Earth.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"### Instruction:\n",
    "Complete the sentence\n",
    "\n",
    "### Input:\n",
    "AI technology is\n",
    "\n",
    "### Response:\n",
    "rapidly advancing and transforming our world.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "train_dataset = Dataset.from_list(training_data)\n",
    "\n",
    "print(f\"✓ Created dataset with {len(train_dataset)} examples\")\n",
    "print(\"\\nSample training example:\")\n",
    "print(\"-\" * 60)\n",
    "print(train_dataset[0]['text'])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Apply LoRA Adapters\n",
    "\n",
    "Add LoRA (Low-Rank Adaptation) adapters to the model for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapters...\n",
      "\n",
      "LoRA configuration set: rank=8, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], dropout=0.05\n",
      "✓ LoRA adapters applied\n",
      "  LoRA rank (r): 8\n",
      "  LoRA alpha: 16\n",
      "  Target modules: q_proj, k_proj, v_proj, o_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahtabsyed/Documents/Claude Code/LLM Finetuning Blog Twin/.venv/lib/python3.13/site-packages/unsloth_mlx/model.py:222: UserWarning: LoRA dropout may have limited support in MLX. Dropout value will be set but behavior may differ from PyTorch PEFT.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying LoRA adapters...\\n\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # LoRA rank (lower for quick testing)\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters applied\")\n",
    "print(f\"  LoRA rank (r): 8\")\n",
    "print(f\"  LoRA alpha: 16\")\n",
    "print(f\"  Target modules: q_proj, k_proj, v_proj, o_proj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Fine-tune the Model\n",
    "\n",
    "Train the model for a few steps with our simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINE-TUNING\n",
      "============================================================\n",
      "Using unsloth-mlx SFTTrainer\n",
      "\n",
      "Trainer initialized:\n",
      "  Output dir: test_output\n",
      "  Adapter path: adapters\n",
      "  Learning rate: 0.0002\n",
      "  Iterations: 15\n",
      "  Batch size: 1\n",
      "  LoRA r=8, alpha=16\n",
      "  Native training: True\n",
      "  LR scheduler: cosine\n",
      "  Grad checkpoint: False\n",
      "Training configuration:\n",
      "  Batch size: 1\n",
      "  Max steps: 15\n",
      "  Learning rate: 2e-4\n",
      "  Dataset size: 3 examples\n",
      "\n",
      "Starting training...\n",
      "\n",
      "======================================================================\n",
      "Starting Fine-Tuning\n",
      "======================================================================\n",
      "\n",
      "[Using Native MLX Training]\n",
      "\n",
      "Applying LoRA adapters...\n",
      "Applying LoRA to 16 layers: {'rank': 8, 'scale': 2.0, 'dropout': 0.05, 'keys': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj']}\n",
      "✓ LoRA applied successfully to 16 layers\n",
      "  Trainable LoRA parameters: 128\n",
      "Preparing training data...\n",
      "  Detected format: text\n",
      "✓ Prepared 3 training samples\n",
      "  Saved to: test_output/train.jsonl\n",
      "✓ Created validation set (copied from train)\n",
      "\n",
      "Training configuration:\n",
      "  Iterations: 15\n",
      "  Batch size: 1\n",
      "  Learning rate: 0.0002\n",
      "  LR scheduler: cosine\n",
      "  Grad checkpoint: True\n",
      "  Adapter file: adapters/adapters.safetensors\n",
      "\n",
      "Loaded 3 training samples, 3 validation samples\n",
      "Starting training loop...\n",
      "Starting training..., iters: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 3/3 [00:00<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 4.820, Val took 0.360s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5: Train loss 2.917, Learning Rate 1.669e-04, It/sec 1.724, Tokens/sec 46.537, Trained Tokens 135, Peak mem 0.802 GB\n",
      "Iter 10: Train loss 0.663, Learning Rate 6.910e-05, It/sec 14.200, Tokens/sec 369.200, Trained Tokens 265, Peak mem 0.802 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 3/3 [00:00<00:00, 36.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15: Val loss 0.423, Val took 0.085s\n",
      "Iter 15: Train loss 0.431, Learning Rate 2.185e-06, It/sec 14.156, Tokens/sec 382.206, Trained Tokens 400, Peak mem 0.802 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "======================================================================\n",
      "  Adapters saved to: adapters\n",
      "\n",
      "============================================================\n",
      "✓ Fine-tuning complete!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import trainer based on platform\n",
    "if platform_type == 'apple_silicon':\n",
    "    try:\n",
    "        from unsloth_mlx import SFTTrainer, SFTConfig\n",
    "        TrainingArgsClass = SFTConfig\n",
    "        print(\"Using unsloth-mlx SFTTrainer\\n\")\n",
    "    except ImportError:\n",
    "        # Fallback to standard trainer\n",
    "        from trl import SFTTrainer\n",
    "        from transformers import TrainingArguments\n",
    "        TrainingArgsClass = TrainingArguments\n",
    "        print(\"Using standard SFTTrainer (fallback)\\n\")\n",
    "else:\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "    TrainingArgsClass = TrainingArguments\n",
    "    print(\"Using standard SFTTrainer\\n\")\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArgsClass(\n",
    "    output_dir=\"./test_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    max_steps=15,  # Very short for quick testing\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    warmup_steps=2,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for this test\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Batch size: 1\")\n",
    "print(f\"  Max steps: 15\")\n",
    "print(f\"  Learning rate: 2e-4\")\n",
    "print(f\"  Dataset size: {len(train_dataset)} examples\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Fine-tuning complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Post-Training Inference (AFTER Fine-tuning)\n",
    "\n",
    "Test the fine-tuned model with the same prompts to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POST-TRAINING INFERENCE (After Fine-tuning)\n",
      "============================================================\n",
      "Inference mode enabled with KV caching\n",
      "\n",
      "Prompt: Complete this sentence: The sky is\n",
      "Response: \n",
      "\n",
      "## Input:\n",
      "The sky is\n",
      "\n",
      "## Response:\n",
      "blue and beautiful, stretching endlessly above us.\n",
      "\n",
      "Prompt: Complete this sentence: Water is\n",
      "Response: \n",
      "\n",
      "Answer: essential for all life on Earth.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING INFERENCE (After Fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test same prompts as before\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    # MLX generate expects prompt string directly, not tokenized inputs\n",
    "    response = model.generate(prompt=prompt, max_tokens=20)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Summary & Comparison\n",
    "\n",
    "Summary of what we tested and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "\n",
      "✓ Platform detected: APPLE_SILICON\n",
      "✓ Library used: unsloth-mlx\n",
      "✓ Base model loaded: mlx-community/Llama-3.2-1B-Instruct-4bit\n",
      "✓ Fine-tuning completed: 15 steps on 3 examples\n",
      "✓ Inference tested: Before and after fine-tuning\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "If fine-tuning worked successfully:\n",
      "1. Update src/finetune.py with GPU detection logic\n",
      "2. Update pyproject.toml with platform-specific dependencies\n",
      "3. Test with your actual blog data\n",
      "4. Deploy the fine-tuned model to Ollama\n",
      "\n",
      "If there were errors:\n",
      "- Review error messages above\n",
      "- Check unsloth-mlx compatibility with your macOS version\n",
      "- Consider using Google Colab for CUDA-based fine-tuning\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n✓ Platform detected: {platform_type.upper()}\")\n",
    "print(f\"✓ Library used: {'unsloth-mlx' if platform_type == 'apple_silicon' else 'unsloth'}\")\n",
    "print(f\"✓ Base model loaded: {model_name}\")\n",
    "print(f\"✓ Fine-tuning completed: 15 steps on 3 examples\")\n",
    "print(f\"✓ Inference tested: Before and after fine-tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf fine-tuning worked successfully:\")\n",
    "print(\"1. Update src/finetune.py with GPU detection logic\")\n",
    "print(\"2. Update pyproject.toml with platform-specific dependencies\")\n",
    "print(\"3. Test with your actual blog data\")\n",
    "print(\"4. Deploy the fine-tuned model to Ollama\")\n",
    "print(\"\\nIf there were errors:\")\n",
    "print(\"- Review error messages above\")\n",
    "print(\"- Check unsloth-mlx compatibility with your macOS version\")\n",
    "print(\"- Consider using Google Colab for CUDA-based fine-tuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-blogging-twin (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
